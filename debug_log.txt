mkdir -p failed for path /home/hmpiao/.config/matplotlib: [Errno 13] Permission denied: '/home/hmpiao/.config/matplotlib'
Matplotlib created a temporary cache directory at /tmp/matplotlib-isgqbrwu because there was an issue with the default path (/home/hmpiao/.config/matplotlib); it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.
[INFO:swift] [LOGO] Patched PEFT Linear.forward for mixture mode support
[INFO:swift] Successfully registered `/data1/hmpiao/jinyike/LoraRetriever-internvl2/swift/llm/data/dataset_info.json`
[INFO:swift] No vLLM installed, if you are using vLLM, you will get `ImportError: cannot import name 'get_vllm_engine' from 'swift.llm'`
[INFO:swift] No LMDeploy installed, if you are using LMDeploy, you will get `ImportError: cannot import name 'prepare_lmdeploy_engine_template' from 'swift.llm'`
[INFO:swift] Global seed set to 42
[INFO:swift] ============================================================
[INFO:swift] LoraRetriever Inference
[INFO:swift] ============================================================
[INFO:swift] Model: qwen2-vl-7b-instruct
[INFO:swift] Model Path: /home/hmpiao/hmpiao/Qwen2-VL-7B-Instruct
[INFO:swift] Merge Method: mixture
[INFO:swift] Top-K: 3
[INFO:swift] Jina Model: /home/hmpiao/hmpiao/jina-embeddings-v4
[INFO:swift] ============================================================
[INFO:swift] Loading test data from: data/Val_100.jsonl
[INFO:swift] Loaded 100 samples
[INFO:swift] Debug mode: processing 2 samples
[INFO:swift] Loaded 19 LoRA configs
[INFO:swift] Initializing LoRA Retriever...
[INFO:swift] Retriever loaded 19 LoRA embeddings
[INFO:swift] Loading base model: qwen2-vl-7b-instruct
[INFO:swift] Loading the model using model_dir: /home/hmpiao/hmpiao/Qwen2-VL-7B-Instruct
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.
[INFO:swift] model_kwargs: {'device_map': 'auto', 'low_cpu_mem_usage': True}
[INFO] Loaded 19 LoRA embeddings
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|â–ˆâ–ˆ        | 1/5 [00:00<00:03,  1.22it/s]Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:01<00:02,  1.31it/s]Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:02<00:01,  1.35it/s]Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:02<00:00,  1.36it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.84it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.57it/s]
[INFO:swift] model.max_model_len: 32768
[INFO:swift] Loading all LoRA adapters...
/data1/hmpiao/tmp/envs/Lretriever/lib/python3.10/site-packages/peft/peft_model.py:598: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.model.visual.blocks.0.attn.qkv.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.0.attn.qkv.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.0.attn.proj.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.0.attn.proj.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.0.mlp.fc1.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.0.mlp.fc1.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.0.mlp.fc2.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.0.mlp.fc2.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.1.attn.qkv.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.1.attn.qkv.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.1.attn.proj.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.1.attn.proj.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.1.mlp.fc1.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.1.mlp.fc1.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.1.mlp.fc2.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.1.mlp.fc2.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.2.attn.qkv.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.2.attn.qkv.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.2.attn.proj.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.2.attn.proj.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.2.mlp.fc1.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.2.mlp.fc1.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.2.mlp.fc2.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.2.mlp.fc2.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.3.attn.qkv.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.3.attn.qkv.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.3.attn.proj.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.3.attn.proj.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.3.mlp.fc1.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.3.mlp.fc1.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.3.mlp.fc2.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.3.mlp.fc2.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.4.attn.qkv.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.4.attn.qkv.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.4.attn.proj.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.4.attn.proj.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.4.mlp.fc1.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.4.mlp.fc1.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.4.mlp.fc2.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.4.mlp.fc2.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.5.attn.qkv.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.5.attn.qkv.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.5.attn.proj.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.5.attn.proj.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.5.mlp.fc1.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.5.mlp.fc1.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.5.mlp.fc2.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.5.mlp.fc2.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.6.attn.qkv.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.6.attn.qkv.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.6.attn.proj.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.6.attn.proj.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.6.mlp.fc1.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.6.mlp.fc1.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.6.mlp.fc2.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.6.mlp.fc2.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.7.attn.qkv.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.7.attn.qkv.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.7.attn.proj.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.7.attn.proj.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.7.mlp.fc1.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.7.mlp.fc1.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.7.mlp.fc2.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.7.mlp.fc2.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.8.attn.qkv.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.8.attn.qkv.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.8.attn.proj.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.8.attn.proj.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.8.mlp.fc1.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.8.mlp.fc1.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.8.mlp.fc2.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.8.mlp.fc2.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.9.attn.qkv.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.9.attn.qkv.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.9.attn.proj.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.9.attn.proj.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.9.mlp.fc1.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.9.mlp.fc1.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.9.mlp.fc2.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.9.mlp.fc2.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.10.attn.qkv.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.10.attn.qkv.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.10.attn.proj.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.10.attn.proj.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.10.mlp.fc1.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.10.mlp.fc1.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.10.mlp.fc2.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.10.mlp.fc2.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.11.attn.qkv.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.11.attn.qkv.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.11.attn.proj.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.11.attn.proj.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.11.mlp.fc1.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.11.mlp.fc1.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.11.mlp.fc2.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.11.mlp.fc2.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.12.attn.qkv.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.12.attn.qkv.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.12.attn.proj.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.12.attn.proj.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.12.mlp.fc1.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.12.mlp.fc1.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.12.mlp.fc2.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.12.mlp.fc2.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.13.attn.qkv.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.13.attn.qkv.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.13.attn.proj.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.13.attn.proj.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.13.mlp.fc1.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.13.mlp.fc1.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.13.mlp.fc2.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.13.mlp.fc2.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.14.attn.qkv.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.14.attn.qkv.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.14.attn.proj.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.14.attn.proj.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.14.mlp.fc1.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.14.mlp.fc1.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.14.mlp.fc2.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.14.mlp.fc2.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.15.attn.qkv.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.15.attn.qkv.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.15.attn.proj.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.15.attn.proj.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.15.mlp.fc1.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.15.mlp.fc1.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.15.mlp.fc2.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.15.mlp.fc2.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.16.attn.qkv.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.16.attn.qkv.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.16.attn.proj.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.16.attn.proj.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.16.mlp.fc1.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.16.mlp.fc1.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.16.mlp.fc2.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.16.mlp.fc2.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.17.attn.qkv.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.17.attn.qkv.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.17.attn.proj.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.17.attn.proj.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.17.mlp.fc1.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.17.mlp.fc1.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.17.mlp.fc2.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.17.mlp.fc2.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.18.attn.qkv.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.18.attn.qkv.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.18.attn.proj.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.18.attn.proj.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.18.mlp.fc1.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.18.mlp.fc1.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.18.mlp.fc2.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.18.mlp.fc2.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.19.attn.qkv.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.19.attn.qkv.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.19.attn.proj.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.19.attn.proj.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.19.mlp.fc1.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.19.mlp.fc1.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.19.mlp.fc2.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.19.mlp.fc2.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.20.attn.qkv.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.20.attn.qkv.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.20.attn.proj.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.20.attn.proj.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.20.mlp.fc1.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.20.mlp.fc1.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.20.mlp.fc2.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.20.mlp.fc2.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.21.attn.qkv.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.21.attn.qkv.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.21.attn.proj.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.21.attn.proj.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.21.mlp.fc1.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.21.mlp.fc1.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.21.mlp.fc2.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.21.mlp.fc2.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.22.attn.qkv.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.22.attn.qkv.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.22.attn.proj.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.22.attn.proj.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.22.mlp.fc1.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.22.mlp.fc1.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.22.mlp.fc2.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.22.mlp.fc2.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.23.attn.qkv.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.23.attn.qkv.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.23.attn.proj.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.23.attn.proj.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.23.mlp.fc1.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.23.mlp.fc1.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.23.mlp.fc2.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.23.mlp.fc2.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.24.attn.qkv.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.24.attn.qkv.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.24.attn.proj.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.24.attn.proj.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.24.mlp.fc1.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.24.mlp.fc1.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.24.mlp.fc2.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.24.mlp.fc2.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.25.attn.qkv.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.25.attn.qkv.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.25.attn.proj.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.25.attn.proj.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.25.mlp.fc1.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.25.mlp.fc1.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.25.mlp.fc2.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.25.mlp.fc2.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.26.attn.qkv.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.26.attn.qkv.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.26.attn.proj.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.26.attn.proj.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.26.mlp.fc1.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.26.mlp.fc1.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.26.mlp.fc2.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.26.mlp.fc2.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.27.attn.qkv.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.27.attn.qkv.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.27.attn.proj.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.27.attn.proj.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.27.mlp.fc1.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.27.mlp.fc1.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.27.mlp.fc2.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.27.mlp.fc2.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.28.attn.qkv.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.28.attn.qkv.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.28.attn.proj.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.28.attn.proj.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.28.mlp.fc1.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.28.mlp.fc1.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.28.mlp.fc2.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.28.mlp.fc2.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.29.attn.qkv.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.29.attn.qkv.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.29.attn.proj.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.29.attn.proj.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.29.mlp.fc1.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.29.mlp.fc1.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.29.mlp.fc2.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.29.mlp.fc2.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.30.attn.qkv.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.30.attn.qkv.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.30.attn.proj.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.30.attn.proj.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.30.mlp.fc1.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.30.mlp.fc1.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.30.mlp.fc2.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.30.mlp.fc2.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.31.attn.qkv.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.31.attn.qkv.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.31.attn.proj.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.31.attn.proj.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.31.mlp.fc1.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.31.mlp.fc1.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.31.mlp.fc2.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.blocks.31.mlp.fc2.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.merger.mlp.0.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.merger.mlp.0.lora_B.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.merger.mlp.2.lora_A.app_lora_adidas_qwen2vl.weight', 'base_model.model.model.visual.merger.mlp.2.lora_B.app_lora_adidas_qwen2vl.weight'].
  warnings.warn(warn_message)
[INFO:swift]   Loaded: app_lora_adidas_qwen2vl
[INFO:swift]   Loaded: app_lora_amazon_qwen2vl
[INFO:swift]   Loaded: app_lora_calendar_qwen2vl
[INFO:swift]   Loaded: app_lora_clock_qwen2vl
[INFO:swift]   Loaded: app_lora_decathlon_qwen2vl
[INFO:swift]   Loaded: app_lora_ebay_qwen2vl
[INFO:swift]   Loaded: app_lora_etsy_qwen2vl
[INFO:swift]   Loaded: app_lora_flipkart_qwen2vl
[INFO:swift]   Loaded: app_lora_gmail_qwen2vl
[INFO:swift]   Loaded: app_lora_google_drive_qwen2vl
[INFO:swift]   Loaded: app_lora_google_maps_qwen2vl
[INFO:swift]   Loaded: app_lora_kitchen_stories_qwen2vl
[INFO:swift]   Loaded: app_lora_reminder_qwen2vl
[INFO:swift]   Loaded: app_lora_youtube_qwen2vl
[INFO:swift]   Loaded: category_lora_entertainment_qwen2vl
[INFO:swift]   Loaded: category_lora_lives_qwen2vl
[INFO:swift]   Loaded: category_lora_office_qwen2vl
[INFO:swift]   Loaded: category_lora_shopping_qwen2vl
[INFO:swift]   Loaded: category_lora_traveling_qwen2vl
[INFO:swift] Loaded 19 LoRA adapters
[INFO:swift] ========================================
[INFO:swift] [DEBUG] Inspecting model layer types:
[INFO:swift]   base_model.model.model.language_model.layers.0.self_attn.q_proj: <class 'peft.tuners.lora.layer.Linear'>
[INFO:swift]     lora_A keys: ['app_lora_adidas_qwen2vl', 'app_lora_amazon_qwen2vl', 'app_lora_calendar_qwen2vl', 'app_lora_clock_qwen2vl', 'app_lora_decathlon_qwen2vl', 'app_lora_ebay_qwen2vl', 'app_lora_etsy_qwen2vl', 'app_lora_flipkart_qwen2vl', 'app_lora_gmail_qwen2vl', 'app_lora_google_drive_qwen2vl', 'app_lora_google_maps_qwen2vl', 'app_lora_kitchen_stories_qwen2vl', 'app_lora_reminder_qwen2vl', 'app_lora_youtube_qwen2vl', 'category_lora_entertainment_qwen2vl', 'category_lora_lives_qwen2vl', 'category_lora_office_qwen2vl', 'category_lora_shopping_qwen2vl', 'category_lora_traveling_qwen2vl']
[INFO:swift]   base_model.model.model.language_model.layers.0.self_attn.q_proj.base_layer: <class 'torch.nn.modules.linear.Linear'>
[INFO:swift]   base_model.model.model.language_model.layers.0.self_attn.q_proj.lora_dropout: <class 'torch.nn.modules.container.ModuleDict'>
[INFO:swift]   base_model.model.model.language_model.layers.0.self_attn.q_proj.lora_dropout.app_lora_adidas_qwen2vl: <class 'torch.nn.modules.dropout.Dropout'>
[INFO:swift]   base_model.model.model.language_model.layers.0.self_attn.q_proj.lora_dropout.app_lora_amazon_qwen2vl: <class 'torch.nn.modules.dropout.Dropout'>
[INFO:swift] ========================================
[INFO:swift] Using mixture composition strategy
[INFO:swift] Starting LoraRetriever inference...
LoraRetriever Inference:   0%|                                                                    | 0/2 [00:00<?, ?it/s][1/2] Retrieving LoRAs:   0%|                                                                     | 0/2 [00:00<?, ?it/s]You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[INFO] Loading jina-embeddings-v4 from /home/hmpiao/hmpiao/jina-embeddings-v4

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s][A
Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  1.59it/s][A
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  2.39it/s][ALoading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  2.22it/s]

Encoding texts...:   0%|          | 0/1 [00:00<?, ?it/s][AThe input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.

Encoding texts...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.78it/s][AEncoding texts...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.77it/s]

Encoding images...:   0%|          | 0/2 [00:00<?, ?it/s][A
Encoding images...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  1.83it/s][AEncoding images...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  3.22it/s]
[1/2] Retrieving LoRAs:   0%|                                    | 0/2 [00:06<?, ?it/s, 8imgs | LoRAs: qwen2vl, qwen2vl]
Encoding texts...:   0%|          | 0/1 [00:00<?, ?it/s][AEncoding texts...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 22.95it/s]

Encoding images...:   0%|          | 0/2 [00:00<?, ?it/s][A
Encoding images...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 12.28it/s][AEncoding images...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 12.25it/s]
[INFO:swift] Sample 0 similarities:
[INFO:swift]   category_lora_lives_qwen2vl: 0.7196
[INFO:swift]   category_lora_entertainment_qwen2vl: 0.6547
[INFO:swift]   category_lora_office_qwen2vl: 0.6493
[INFO:swift]   app_lora_youtube_qwen2vl: 0.6456
[INFO:swift]   app_lora_reminder_qwen2vl: 0.6356
[INFO:swift]   category_lora_traveling_qwen2vl: 0.6260
[INFO:swift]   app_lora_clock_qwen2vl: 0.6169
[INFO:swift]   category_lora_shopping_qwen2vl: 0.6122
[INFO:swift]   app_lora_google_maps_qwen2vl: 0.6120
[INFO:swift]   app_lora_calendar_qwen2vl: 0.6116
[INFO:swift]   app_lora_google_drive_qwen2vl: 0.6072
[INFO:swift]   app_lora_kitchen_stories_qwen2vl: 0.6037
[INFO:swift]   app_lora_decathlon_qwen2vl: 0.6032
[INFO:swift]   app_lora_gmail_qwen2vl: 0.6005
[INFO:swift]   app_lora_amazon_qwen2vl: 0.5934
[INFO:swift]   app_lora_etsy_qwen2vl: 0.5908
[INFO:swift]   app_lora_flipkart_qwen2vl: 0.5846
[INFO:swift]   app_lora_adidas_qwen2vl: 0.5791
[INFO:swift]   app_lora_ebay_qwen2vl: 0.5761
[1/2] Inferencing:   0%|                                         | 0/2 [00:06<?, ?it/s, 8imgs | LoRAs: qwen2vl, qwen2vl][INFO:swift] Setting image_factor: 16. You can adjust this hyperparameter through the environment variable: `IMAGE_FACTOR`.
[INFO:swift] Setting resized_height: None. You can adjust this hyperparameter through the environment variable: `RESIZED_HEIGHT`.
[INFO:swift] Setting resized_width: None. You can adjust this hyperparameter through the environment variable: `RESIZED_WIDTH`.
[INFO:swift] Setting min_pixels: 576. You can adjust this hyperparameter through the environment variable: `MIN_PIXELS`.
[INFO:swift] Using environment variable `MAX_PIXELS`, Setting max_pixels: 100000.
[INFO:swift] LOGO Mixture mode enabled: adapters=['app_lora_adidas_qwen2vl', 'app_lora_amazon_qwen2vl', 'app_lora_calendar_qwen2vl', 'app_lora_clock_qwen2vl', 'app_lora_decathlon_qwen2vl', 'app_lora_ebay_qwen2vl', 'app_lora_etsy_qwen2vl', 'app_lora_flipkart_qwen2vl', 'app_lora_gmail_qwen2vl', 'app_lora_google_drive_qwen2vl', 'app_lora_google_maps_qwen2vl', 'app_lora_kitchen_stories_qwen2vl', 'app_lora_reminder_qwen2vl', 'app_lora_youtube_qwen2vl', 'category_lora_entertainment_qwen2vl', 'category_lora_lives_qwen2vl', 'category_lora_office_qwen2vl', 'category_lora_shopping_qwen2vl', 'category_lora_traveling_qwen2vl'], weights shape=torch.Size([1, 19])
[1/2] âœ“ Done:   0%|                                              | 0/2 [00:20<?, ?it/s, 8imgs | LoRAs: qwen2vl, qwen2vl][INFO:swift] 
Sample 0:
[INFO:swift]   Query: <image>
<image>
<image>
<image>
<image>
<image>
<image>
<image>
In the Balance app, I want to listen...
[INFO:swift]   Selected: [('category_lora_lives_qwen2vl', '0.412'), ('category_lora_entertainment_qwen2vl', '0.298'), ('category_lora_office_qwen2vl', '0.290')]
[INFO:swift]   Response: To listen to the "Along the River" meditation in the Balance app, follow these steps:

1. Open the Balance app on your device.
2. Tap on the "Today" tab at the bottom of the screen.
3. Scroll down to ...
[1/2] âœ“ Done:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                   | 1/2 [00:20<00:20, 20.84s/it, 8imgs | LoRAs: qwen2vl, qwen2vl][2/2] Retrieving LoRAs:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ              | 1/2 [00:20<00:20, 20.84s/it, 8imgs | LoRAs: qwen2vl, qwen2vl]
Encoding texts...:   0%|          | 0/1 [00:00<?, ?it/s][AEncoding texts...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 21.75it/s]

Encoding images...:   0%|          | 0/2 [00:00<?, ?it/s][A
Encoding images...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  9.36it/s][AEncoding images...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 10.32it/s]
[2/2] Retrieving LoRAs:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ              | 1/2 [00:21<00:20, 20.84s/it, 2imgs | LoRAs: qwen2vl, qwen2vl]
Encoding texts...:   0%|          | 0/1 [00:00<?, ?it/s][AEncoding texts...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 21.25it/s]

Encoding images...:   0%|          | 0/2 [00:00<?, ?it/s][A
Encoding images...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 12.21it/s][AEncoding images...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 12.19it/s]
[INFO:swift] Sample 1 similarities:
[INFO:swift]   app_lora_youtube_qwen2vl: 0.8212
[INFO:swift]   category_lora_entertainment_qwen2vl: 0.7197
[INFO:swift]   category_lora_lives_qwen2vl: 0.6737
[INFO:swift]   app_lora_kitchen_stories_qwen2vl: 0.6584
[INFO:swift]   category_lora_office_qwen2vl: 0.6531
[INFO:swift]   category_lora_shopping_qwen2vl: 0.6495
[INFO:swift]   app_lora_flipkart_qwen2vl: 0.6492
[INFO:swift]   app_lora_ebay_qwen2vl: 0.6376
[INFO:swift]   app_lora_gmail_qwen2vl: 0.6370
[INFO:swift]   app_lora_etsy_qwen2vl: 0.6359
[INFO:swift]   app_lora_amazon_qwen2vl: 0.6350
[INFO:swift]   app_lora_google_drive_qwen2vl: 0.6337
[INFO:swift]   app_lora_decathlon_qwen2vl: 0.6319
[INFO:swift]   app_lora_adidas_qwen2vl: 0.6256
[INFO:swift]   app_lora_google_maps_qwen2vl: 0.6169
[INFO:swift]   app_lora_reminder_qwen2vl: 0.6148
[INFO:swift]   category_lora_traveling_qwen2vl: 0.6146
[INFO:swift]   app_lora_calendar_qwen2vl: 0.5942
[INFO:swift]   app_lora_clock_qwen2vl: 0.5869
[2/2] Inferencing:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                | 1/2 [00:21<00:20, 20.84s/it, 2imgs | LoRAs: qwen2vl, qwen2vl][INFO:swift] LOGO Mixture mode enabled: adapters=['app_lora_adidas_qwen2vl', 'app_lora_amazon_qwen2vl', 'app_lora_calendar_qwen2vl', 'app_lora_clock_qwen2vl', 'app_lora_decathlon_qwen2vl', 'app_lora_ebay_qwen2vl', 'app_lora_etsy_qwen2vl', 'app_lora_flipkart_qwen2vl', 'app_lora_gmail_qwen2vl', 'app_lora_google_drive_qwen2vl', 'app_lora_google_maps_qwen2vl', 'app_lora_kitchen_stories_qwen2vl', 'app_lora_reminder_qwen2vl', 'app_lora_youtube_qwen2vl', 'category_lora_entertainment_qwen2vl', 'category_lora_lives_qwen2vl', 'category_lora_office_qwen2vl', 'category_lora_shopping_qwen2vl', 'category_lora_traveling_qwen2vl'], weights shape=torch.Size([1, 19])
[2/2] âœ“ Done:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                   | 1/2 [00:40<00:20, 20.84s/it, 2imgs | LoRAs: qwen2vl, qwen2vl][INFO:swift] 
Sample 1:
[INFO:swift]   Query: <image>
<image>
Play a YouTube shorts video on the YouTube app....
[INFO:swift]   Selected: [('app_lora_youtube_qwen2vl', '0.481'), ('category_lora_entertainment_qwen2vl', '0.289'), ('category_lora_lives_qwen2vl', '0.230')]
[INFO:swift]   Response: To play a YouTube shorts video on the YouTube app, follow these steps:

1. Open the YouTube app on your device.
2. Tap on the "Shorts" icon located at the bottom of the screen. This is usually represe...
[2/2] âœ“ Done: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:40<00:00, 19.85s/it, 2imgs | LoRAs: qwen2vl, qwen2vl][2/2] âœ“ Done: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:40<00:00, 20.00s/it, 2imgs | LoRAs: qwen2vl, qwen2vl]
[INFO:swift] 
================================================================================
[INFO:swift] æŽ¨ç†å®Œæˆï¼
[INFO:swift] ================================================================================
[INFO:swift]   æ€»æ ·æœ¬æ•°:   2
[INFO:swift]   æˆåŠŸ:       2 (100.0%)
[INFO:swift]   å¤±è´¥:       0
[INFO:swift]   åˆå¹¶æ–¹æ³•:   mixture
[INFO:swift]   Top-K:      3
[INFO:swift]   ç»“æžœä¿å­˜:   output/lora_retriever_results/retriever_results_qwen2vl_mixture_k3_20260207-235012.jsonl
[INFO:swift] 
LoRA ä½¿ç”¨ç»Ÿè®¡ (Top 10):
[INFO:swift]   lives_qwen2vl       :   2 æ¬¡ (100.0%)
[INFO:swift]   entertainment_qwen2vl:   2 æ¬¡ (100.0%)
[INFO:swift]   office_qwen2vl      :   1 æ¬¡ (50.0%)
[INFO:swift]   youtube_qwen2vl     :   1 æ¬¡ (50.0%)
[INFO:swift] ================================================================================

