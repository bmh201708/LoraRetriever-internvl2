{
  "permissions": {
    "allow": [
      "Bash(find:*)",
      "Bash(CUDA_VISIBLE_DEVICES=2 python3 -c \"\nimport os, sys, torch\nos.environ[''''MAX_PIXELS''''] = ''''100000''''\nos.environ[''''MAX_NUM''''] = ''''12''''\nos.environ.setdefault\\(''''PYTORCH_CUDA_ALLOC_CONF'''', ''''expandable_segments:True''''\\)\n\nsys.path.insert\\(0, ''''.''''\\)\nsys.path.insert\\(0, ''''swift''''\\)\n\nfrom swift.llm.utils import get_model_tokenizer\nfrom swift.tuners import Swift\n\n# 1. 加载 base model\nprint\\(''''=== 1. 加载 base model ===''''\\)\nmodel, tokenizer = get_model_tokenizer\\(\n    ''''qwen2-vl-7b-instruct'''', torch.float16,\n    {''''device_map'''': ''''auto'''', ''''low_cpu_mem_usage'''': True},\n    model_id_or_path=''''/home/hmpiao/hmpiao/Qwen2-VL-7B-Instruct''''\n\\)\nprint\\(f''''Model type: {type\\(model\\).__name__}''''\\)\nprint\\(f''''is_multimodal: {getattr\\(model, \"\"is_multimodal\"\", \"\"NOT_SET\"\"\\)}''''\\)\n\n# 2. 加载第一个 LoRA\nlora_path = ''''/home/hmpiao/hmpiao/jinyike/FedMABench/qwen2_vl_lora_app/qwen2_vl_app_lora_adidas/qwen2-vl-7b-instruct/v0-20260131-111220/global_lora_10''''\nprint\\(f''''\\\\n=== 2. 加载第一个 LoRA: adidas ===''''\\)\nmodel = Swift.from_pretrained\\(model, lora_path, adapter_name=''''app_lora_adidas_qwen2vl'''', inference_mode=True\\)\nprint\\(f''''Model type after LoRA: {type\\(model\\).__name__}''''\\)\n\n# 3. 检查 LoRA 层是否存在\nprint\\(''''\\\\n=== 3. 检查 LoRA 层 ===''''\\)\nfrom peft.tuners.lora import LoraLayer\nlora_count = 0\nadapter_found = 0\nfor name, mod in model.named_modules\\(\\):\n    if isinstance\\(mod, LoraLayer\\):\n        lora_count += 1\n        if ''''app_lora_adidas_qwen2vl'''' in getattr\\(mod, ''''lora_A'''', {}\\):\n            adapter_found += 1\n            if lora_count <= 3:  # 只打印前3个\n                lora_A = mod.lora_A[''''app_lora_adidas_qwen2vl'''']\n                lora_B = mod.lora_B[''''app_lora_adidas_qwen2vl'''']\n                a_norm = lora_A.weight.float\\(\\).norm\\(\\).item\\(\\)\n                b_norm = lora_B.weight.float\\(\\).norm\\(\\).item\\(\\)\n                print\\(f''''  {name}: A_norm={a_norm:.4f}, B_norm={b_norm:.4f}''''\\)\n\nprint\\(f''''\\\\n总 LoRA 层数: {lora_count}''''\\)\nprint\\(f''''包含 adidas adapter 的层数: {adapter_found}''''\\)\n\n# 4. 检查 active adapters\nprint\\(''''\\\\n=== 4. Active adapters ===''''\\)\nif hasattr\\(model, ''''active_adapters''''\\):\n    print\\(f''''active_adapters: {model.active_adapters}''''\\)\nif hasattr\\(model, ''''active_adapter''''\\):\n    print\\(f''''active_adapter: {model.active_adapter}''''\\)\n\n# 5. 加载第二个 LoRA 并检查\nlora_path2 = ''''/home/hmpiao/hmpiao/jinyike/FedMABench/qwen2_vl_lora_app/qwen2_vl_app_lora_amazon/qwen2-vl-7b-instruct/v0-20260130-190721/global_lora_10''''\nprint\\(f''''\\\\n=== 5. 加载第二个 LoRA: amazon ===''''\\)\nmodel = Swift.from_pretrained\\(model, lora_path2, adapter_name=''''app_lora_amazon_qwen2vl'''', inference_mode=True\\)\namazon_found = 0\nfor name, mod in model.named_modules\\(\\):\n    if isinstance\\(mod, LoraLayer\\) and ''''app_lora_amazon_qwen2vl'''' in getattr\\(mod, ''''lora_A'''', {}\\):\n        amazon_found += 1\n        if amazon_found <= 2:\n            lora_A = mod.lora_A[''''app_lora_amazon_qwen2vl'''']\n            lora_B = mod.lora_B[''''app_lora_amazon_qwen2vl'''']\n            a_norm = lora_A.weight.float\\(\\).norm\\(\\).item\\(\\)\n            b_norm = lora_B.weight.float\\(\\).norm\\(\\).item\\(\\)\n            print\\(f''''  {name}: A_norm={a_norm:.4f}, B_norm={b_norm:.4f}''''\\)\nprint\\(f''''包含 amazon adapter 的层数: {amazon_found}''''\\)\n\")"
    ]
  }
}
